{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419360c-2590-478f-905c-daae53d31091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# ファイル名を実際の訓練・テストデータファイル名に置き換えてください\n",
    "train_data_xgb = pd.read_csv('selected_train_set.csv')\n",
    "test_data_xgb = pd.read_csv('selected_test_set.csv')\n",
    "index_col=['id']\n",
    "# 特徴量（X）と目標変数（y）の設定\n",
    "X_train_xgb = train_data_xgb.drop(columns=['PassengerId','Survived'])  # 'target_column'を目標変数の列名に置き換えてください\n",
    "y_train_xgb = train_data_xgb['Survived']\n",
    "X_test_xgb = test_data_xgb.drop(columns=['PassengerId','Survived'])\n",
    "y_test_xgb = test_data_xgb['Survived']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# XGBoostモデルのインスタンス作成\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# パラメータの分布を設定\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],       # 試すブーストラウンド数\n",
    "    'learning_rate': [0.01, 0.05, 0.1],    # 試す学習率\n",
    "    'max_depth': [3, 4, 5],                # 木の最大深さ\n",
    "    'subsample': [0.8, 1.0],               # サブサンプリング比率\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],   # 特徴量のサブサンプリング比率\n",
    "    'gamma': [0, 0.1, 0.2]                 # 正則化項\n",
    "}\n",
    "\n",
    "# RandomizedSearchCVの設定\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model, param_distributions=param_dist, n_iter=10,  # 試行回数\n",
    "    scoring='accuracy', cv=5, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# 最適なパラメータの探索\n",
    "random_search.fit(X_train_xgb, y_train_xgb)\n",
    "\n",
    "# 最適なパラメータの表示\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {random_search.best_score_:.2f}\")\n",
    "\n",
    "# 最適なパラメータでのモデル評価\n",
    "best_xgb_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c45e8-da4a-4a86-969d-10ea98fd01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスの不均衡を考慮した重みの計算\n",
    "ratio = y_train_xgb.value_counts()[0] / y_train_xgb.value_counts()[1]  # 負例/正例の比率\n",
    "\n",
    "\n",
    "# DMatrixに変換\n",
    "dtrain_xgb = xgb.DMatrix(X_train_xgb, label=y_train_xgb)\n",
    "for i in range(10):\n",
    "# パラメータ設定\n",
    "    params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'gamma': 0.1\n",
    "}\n",
    "\n",
    "# クロスバリデーション\n",
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=dtrain_xgb,\n",
    "    num_boost_round=200,  # 最大のブーストラウンド数\n",
    "    nfold=5,  # 5分割でのクロスバリデーション\n",
    "    early_stopping_rounds=10,  # 10回のラウンドで改善がなければ停止\n",
    "    metrics='logloss',  # 評価指標\n",
    "    seed=42,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# 最適なブーストラウンド数を取得\n",
    "optimal_boost_rounds = len(cv_results)\n",
    "print(f\"Optimal number of boosting rounds: {optimal_boost_rounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4e945-b70b-438f-bb66-11e03b2186b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# クロスバリデーションで得た最適なブーストラウンド数\n",
    "optimal_boost_rounds = 28  # ここで得た値\n",
    "\n",
    "# クラスの不均衡を考慮した重みの計算\n",
    "ratio = y_train_xgb.value_counts()[0] / y_train_xgb.value_counts()[1]  # 負例/正例の比率\n",
    "\n",
    "# 10回ループの準備\n",
    "accuracies_xgb = []  # 精度を格納するリスト\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    # 訓練-検証の分割\n",
    "    X_train_split_xgb, X_val_split_xgb, y_train_split_xgb, y_val_split_xgb = train_test_split(X_train_xgb, y_train_xgb, test_size=0.2, random_state=42)\n",
    "\n",
    "#以下に新しく提案されたコードを各\n",
    "    # DMatrixに変換\n",
    "    dtrain_xgb = xgb.DMatrix(X_train_split_xgb, label=y_train_split_xgb)\n",
    "    dval_xgb = xgb.DMatrix(X_val_split_xgb, label=y_val_split_xgb)\n",
    "    dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "\n",
    "    params = {\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'logloss',\n",
    "        'random_state':42,\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth':5,\n",
    "        'subsample':1.0,\n",
    "        'colsample_bytree':1.0,\n",
    "        'gamma':0.1\n",
    "}\n",
    "\n",
    "    \n",
    "    # アーリーストッピングでモデルを訓練\n",
    "    evals_xgb = [(dtrain_xgb, 'train'), (dval_xgb, 'eval')]\n",
    "    model_xgb = xgb.train(params, dtrain_xgb, num_boost_round=optimal_boost_rounds, early_stopping_rounds=10, evals=evals_xgb, verbose_eval=False)\n",
    "    #num_boost_roundを100(0.51)から200に、learn_rateを0.1から0.05に（過学習抑制）、max_depthを3からまずは4に変更\n",
    "    # 予測\n",
    "    y_test_pred_xgb = model_xgb.predict(dtest_xgb)\n",
    "    y_test_pred_xgb = [1 if pred > 0.5 else 0 for pred in y_test_pred_xgb]  # 二値分類として閾値0.5で予測値を変換\n",
    "    accuracy_xgb = accuracy_score(y_test_xgb, y_test_pred_xgb)\n",
    "    accuracies_xgb.append(accuracy_xgb)\n",
    "\n",
    "# 平均精度を計算\n",
    "average_accuracy_xgb = np.mean(accuracies_xgb)\n",
    "print(f'Average Accuracy over 10 runs: {average_accuracy_xgb:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22597c35-c590-492b-8176-343e6f5fbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 特徴量の重要度を取得 (weightとgain)\n",
    "importance_weight_xgb = model_xgb.get_score(importance_type='weight')\n",
    "\n",
    "# 全ての特徴量を取得してデータフレームにまとめる\n",
    "all_features_xgb = X_train_split_xgb.columns\n",
    "importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': all_features_xgb,\n",
    "    'Weight': [importance_weight_xgb.get(feature, 0) for feature in all_features_xgb],  # Weightがない場合は0\n",
    "})\n",
    "\n",
    "\n",
    "# データフレームにまとめて表示\n",
    "importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': list(importance_weight_xgb.keys()),\n",
    "    'Weight': list(importance_weight_xgb.values()),\n",
    "   \n",
    "})\n",
    "\n",
    "# Weightで上位13項目をソート\n",
    "top_13_weight_df_xgb = importance_df_xgb.sort_values(by='Weight', ascending=False).head(13)\n",
    "\n",
    "\n",
    "# 結果の表示\n",
    "print(\"Top 13 Feature Importances by Weight:\\n\", top_13_weight_df_xgb)\n",
    "\n",
    "\n",
    "# グラフの作成\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Weightのプロット\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(top_13_weight_df_xgb['Feature'], top_13_weight_df_xgb['Weight'], color='lightgreen')\n",
    "plt.xlabel('Weight')\n",
    "plt.title('Top 13 Feature Importances by Weight')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e80950-2a9e-4458-9b02-7d5968fc7762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,confusion_matrix\n",
    "\n",
    "# テストデータをDMatrix形式に変換\n",
    "dtest = xgb.DMatrix(X_test_xgb)\n",
    "# テストデータでの予測確率を取得\n",
    "y_scores_xgb = model_xgb.predict(dtest_xgb)\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_xgb = np.sort(np.unique(y_scores_xgb))\n",
    "\n",
    "# 感度（TPR）と特異度（1 - FPR）を計算\n",
    "tpr_list_xgb = []\n",
    "specificity_list_xgb = []\n",
    "\n",
    "for threshold in thresholds_xgb:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_xgb = (y_scores_xgb >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_xgb, fp_xgb, fn_xgb, tp_xgb = confusion_matrix(y_test_xgb, y_pred_xgb).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_xgb = tp_xgb / (tp_xgb + fn_xgb) if (tp_xgb + fn_xgb) > 0 else 0  # 感度\n",
    "    specificity_xgb = tn_xgb / (tn_xgb + fp_xgb) if (tn_xgb + fp_xgb) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_xgb.append(tpr_xgb)\n",
    "    specificity_list_xgb.append(specificity_xgb)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_xgb = auc(1 - np.array(specificity_list_xgb), tpr_list_xgb)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.array(specificity_list_xgb), tpr_list_xgb, label=f\"ROC Curve (AUC = {roc_auc_xgb:.2f})\", color=\"blue\")\n",
    "plt.plot([1, 0], [0, 1], color=\"gray\", linestyle=\"--\")  # 45度線\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に設定\n",
    "plt.ylim([0.0, 1.0])  # Y軸を0から1に設定\n",
    "plt.xlabel(\"Specificity \")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve (Specificity vs Sensitivity)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d0dfe-ffaf-433f-be46-4b367e4ff2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# テストデータでの予測\n",
    "y_pred_xgb = model_xgb.predict(dtest_xgb)\n",
    "# 二値分類のため、0.5を閾値として0または1に変換\n",
    "y_pred_xgb = [1 if pred > 0.5 else 0 for pred in y_pred_xgb]\n",
    "\n",
    "# 混同行列の計算\n",
    "cm = confusion_matrix(y_test_xgb, y_pred_xgb)\n",
    "\n",
    "# 混同行列の表示\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 混同行列の要素を使って感度と特異度を計算\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)  # 感度\n",
    "specificity = tn / (tn + fp)  # 特異度\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad1242-2132-4a07-acf5-c7b1f1c8ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 最初の決定木を可視化する\n",
    "xgb.plot_tree(model_xgb, num_trees=0)\n",
    "plt.show()\n",
    "# 決定木の可視化設定\n",
    "fig, ax = plt.subplots(figsize=(20, 10))  # 図のサイズを大きく設定\n",
    "\n",
    "# Graphvizの属性を辞書形式で設定\n",
    "graph_attr = {'rankdir': 'LR', 'size': '20,10'}  # 'size'は図のサイズを指定（単位はインチ）\n",
    "\n",
    "\n",
    "xgb.plot_tree(model_xgb, num_trees=0, ax=ax, **{'graph_attr': graph_attr})  # フォントサイズを指定\n",
    "plt.show()\n",
    "\n",
    "# 図をファイルとして保存\n",
    "fig.savefig('new_tree_visualization.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85508a1f-d267-4570-9908-363a7b6f07bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a93f19-9892-4208-9d31-54863b25f875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
