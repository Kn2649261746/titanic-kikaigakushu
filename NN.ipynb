{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908daf8e-ab47-4f77-a0ab-6bfb6b5f4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの読み込み\n",
    "train_data_nn = pd.read_csv('selected_train_set.csv')\n",
    "test_data_nn = pd.read_csv('selected_test_set.csv')\n",
    "\n",
    "# 特徴量と目的変数の分離\n",
    "X_train_nn = train_data_nn.drop(columns=['PassengerId','Survived'])\n",
    "y_train_nn = train_data_nn['Survived']\n",
    "X_test_nn = test_data_nn.drop(columns=['PassengerId','Survived'])\n",
    "y_test_nn = test_data_nn['Survived']\n",
    "\n",
    "# データの標準化\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_scaled_nn = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_scaled_nn = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# 標準化されたデータをデータフレームに戻し、元のカラム名を保持\n",
    "X_train_scaled_df_nn = pd.DataFrame(X_train_scaled_nn, columns=X_train_nn.columns, index=X_train_nn.index)\n",
    "X_test_scaled_df_nn = pd.DataFrame(X_test_scaled_nn, columns=X_test_nn.columns, index=X_test_nn.index)\n",
    "# ニューラルネットワークのパラメータグリッドの設定\n",
    "param_grid_nn = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # 隠れ層の構成\n",
    "    'activation': ['relu', 'tanh'],                   # 活性化関数\n",
    "    'learning_rate_init': [0.001, 0.01],              # 学習率\n",
    "    'max_iter': [200, 500]                            # 最大エポック数\n",
    "}\n",
    "\n",
    "# 重要度を格納するリスト\n",
    "importances_list_nn = []\n",
    "# 予測確率を格納するリスト\n",
    "predicted_probabilities_nn = []\n",
    "# 精度を格納するリスト\n",
    "accuracy_scores_nn = []\n",
    "\n",
    "# モデルのトレーニングと評価を10回繰り返す\n",
    "for i in range(10):\n",
    "    # ニューラルネットワークモデルの初期化\n",
    "    model_nn = MLPClassifier(random_state=42)\n",
    "\n",
    "\n",
    "    # GridSearchCVの設定\n",
    "    grid_search_nn = GridSearchCV(estimator=model_nn, param_grid=param_grid_nn, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# 訓練データでモデルをトレーニングし、最適なハイパーパラメータを取得\n",
    "    grid_search_nn.fit(X_train_scaled_df_nn, y_train_nn)\n",
    "    best_model_nn = grid_search_nn.best_estimator_\n",
    "\n",
    "    \n",
    "    # 訓練データでモデルをトレーニング\n",
    "    #model_nn.fit(X_train_scaled_df_nn, y_train_nn)\n",
    "\n",
    "    # 特徴量の重要度を取得し、リストに追加\n",
    "    #importances_list_nn.append(best_model_nn.feature_importances_)\n",
    "\n",
    "    # テストデータで予測確率を取得し、リストに追加\n",
    "    y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "    predicted_probabilities_nn.append(y_scores_nn)\n",
    "\n",
    "    # テストデータで予測を行う\n",
    "    y_pred_nn = best_model_nn.predict(X_test_scaled_df_nn)\n",
    "# Permutation Importanceの計算\n",
    "    result_nn = permutation_importance(best_model_nn, X_test_scaled_df_nn, y_test_nn, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    importances_list_nn.append(result_nn.importances_mean)\n",
    "\n",
    "    # テストデータで予測確率を取得し、リストに追加\n",
    "    y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "    predicted_probabilities_nn.append(y_scores_nn)\n",
    "\n",
    "    # テストデータで予測を行う\n",
    "    y_pred_nn = best_model_nn.predict(X_test_scaled_df_nn)\n",
    "    \n",
    "# 精度を計算してリストに追加\n",
    "    accuracy_nn = accuracy_score(y_test_nn, y_pred_nn)\n",
    "    accuracy_scores_nn.append(accuracy_nn)\n",
    "\n",
    "# 10回の平均精度を計算\n",
    "average_accuracy_nn = np.mean(accuracy_scores_nn)\n",
    "\n",
    "# 平均精度を出力\n",
    "print(f\"\\n平均精度 (10回): {average_accuracy_nn:.4f}\")\n",
    "\n",
    "\n",
    "# 特徴量の重要度の平均を計算\n",
    "mean_importances_nn = np.mean(importances_list_nn, axis=0)\n",
    "\n",
    "# 特徴量の名前と重要度の平均をデータフレームにまとめる\n",
    "feature_importance_df_nn = pd.DataFrame({\n",
    "    'Feature': X_train_nn.columns,\n",
    "    'Importance': mean_importances_nn\n",
    "})\n",
    "\n",
    "# 重要度でソート\n",
    "feature_importance_df_nn = feature_importance_df_nn.sort_values(by='Importance', ascending=False)\n",
    "# 上位13項目を抽出\n",
    "top_13_importance_df_nn = feature_importance_df_nn.head(13)\n",
    "\n",
    "# グラフの作成\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_13_importance_df_nn['Feature'], top_13_importance_df_nn['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 13 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# 平均予測確率を計算\n",
    "mean_predicted_probabilities_nn = np.mean(predicted_probabilities_nn, axis=0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "\n",
    "# テストデータで予測確率を取得\n",
    "y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_nn = np.sort(np.unique(y_scores_nn))\n",
    "\n",
    "# 感度（TPR）と特異度（1 - FPR）を計算\n",
    "tpr_list_nn = []\n",
    "specificity_list_nn = []\n",
    "\n",
    "for threshold in thresholds_nn:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_nn = (y_scores_nn >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_nn, fp_nn, fn_nn, tp_nn = confusion_matrix(y_test_nn, y_pred_nn).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_nn = tp_nn / (tp_nn+ fn_nn) if (tp_nn + fn_nn) > 0 else 0  # 感度\n",
    "    specificity_nn = tn_nn / (tn_nn + fp_nn) if (tn_nn + fp_nn) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_nn.append(tpr_nn)\n",
    "    specificity_list_nn.append(specificity_nn)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_nn = auc(np.array(specificity_list_nn), tpr_list_nn)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( np.array(specificity_list_nn), tpr_list_nn, label=f\"ROC Curve (AUC = {roc_auc_nn:.2f})\", color=\"blue\")\n",
    "\n",
    "plt.plot([1, 0], [0, 1], linestyle=\"--\", color=\"red\", label=\"Random Guess\")\n",
    "\n",
    "# グラフの装飾\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に反転\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"Specificity \")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve with TP, FP, TN, FN\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# 混同行列の計算\n",
    "tn_nn, fp_nn, fn_nn, tp_nn = confusion_matrix(y_test_nn, y_pred_nn).ravel()\n",
    "\n",
    "# 感度と特異度の計算\n",
    "sensitivity_nn = tp_nn / (tp_nn + fn_nn)  # 真陽性率\n",
    "specificity_nn = tn_nn / (tn_nn + fp_nn)  # 真陰性率\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity_nn:.4f}\")\n",
    "print(f\"Specificity: {specificity_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5581acf-c6bb-4daf-aaa8-4806059cce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# 混同行列の計算\n",
    "cm_nn = confusion_matrix(y_test_nn, y_pred_nn)\n",
    "\n",
    "# 混同行列の表示\n",
    "disp_nn = ConfusionMatrixDisplay(confusion_matrix=cm_nn, display_labels=[\"Negative\", \"Positive\"])\n",
    "disp_nn.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 混同行列の要素を使って感度と特異度を計算\n",
    "tn_nn, fp_nn, fn_nn, tp_nn = cm_nn.ravel()\n",
    "sensitivity_nn = tp_nn / (tp_nn + fn_nn)  # 感度\n",
    "specificity_nn = tn_nn / (tn_nn + fp_nn)  # 特異度\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity_nn:.2f}\")\n",
    "print(f\"Specificity: {specificity_nn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73353155-6c7a-413a-b06c-14435e78b5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e0906-7b16-475b-af08-d987a0c8e037",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
