{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f6ada-6c8f-448a-84e0-1351b9e81d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler #標準化のコード\n",
    "\n",
    "# データの読み込み\n",
    "# ファイル名を実際の訓練・テストデータファイル名に置き換えてください\n",
    "train_data_LR = pd.read_csv('selected_train_set.csv')\n",
    "test_data_LR = pd.read_csv('selected_test_set.csv')\n",
    "#index_col=['id']\n",
    "\n",
    "# 特徴量（X）と目標変数（y）の設定\n",
    "#11/1のミーティングで決まった項目も抜いて機械学習\n",
    "\n",
    "X_train_LR = train_data_LR.drop(columns=['PassengerId','Survived'])  # 'target_column'を目標変数の列名に置き換えてください\n",
    "y_train_LR = train_data_LR['Survived']\n",
    "X_test_LR = test_data_LR.drop(columns=['PassengerId','Survived'])\n",
    "y_test_LR = test_data_LR['Survived']\n",
    "\n",
    "# 特徴量の数を確認\n",
    "num_features_LR = X_train_LR.shape[1]\n",
    "print(\"特徴量の数:\", num_features_LR)\n",
    "\n",
    "# 標準化\n",
    "scaler = StandardScaler()\n",
    "X_train_standard_LR = scaler.fit_transform(X_train_LR)\n",
    "X_test_standard_LR = scaler.transform(X_test_LR)\n",
    "# 10回ループのためのリストを初期化\n",
    "accuracies_LR = []  # 精度を格納するリスト\n",
    "# 重要度を格納するリスト\n",
    "importances_list_LR = []\n",
    "\n",
    "predicted_probabilities_LR = []  # 予測確率を格納するリスト\n",
    "random_state = 42\n",
    "# 10回ループでモデルを作成\n",
    "for i in range(10):\n",
    "    # sagaソルバーとElastic Netペナルティを使用したロジスティック回帰モデルを定義\n",
    "    model_LR = LogisticRegression(\n",
    "    solver='saga',           # 小さなデータセットに最適なソルバーで正則化対応\n",
    "    penalty='elasticnet',    # Elastic Netで特徴選択と正則化を両立\n",
    "    l1_ratio=0.5,            # L1とL2のバランス（0.5は出発点として適切）\n",
    "    C=1.0,                   # 正則化の強度（調整やチューニングも可能）\n",
    "    max_iter=10000,            # 収束を確実にするために最大イテレーションを増加   #5000だと精度0.50、\n",
    "    random_state=random_state  # Set random state for reproducibility\n",
    ")\n",
    "    # ロジスティック回帰モデルの作成\n",
    "    #model = LogisticRegression(max_iter=100, random_state=i)  # max_iterで反復回数を調整 #100で0.50,200で0.44\n",
    "    #model = LogisticRegression(solver='liblinear')#0.44だった\n",
    "\n",
    "    # モデルの訓練\n",
    "    model_LR.fit(X_train_standard_LR, y_train_LR)\n",
    "\n",
    "# 回帰係数を取得（11/1）\n",
    "    coefficients_LR = model_LR.coef_[0]  # 係数は配列として返されるので、最初の要素を取得\n",
    "    \n",
    "    # テストデータでの予測\n",
    "    y_pred_LR = model_LR.predict(X_test_standard_LR)\n",
    "\n",
    "    # 特徴量の重要度を取得し、リストに追加 11/1\n",
    "    importances_list_LR.append(np.abs(coefficients_LR))  # 絶対値を取って追加\n",
    "\n",
    "    # テストデータで予測確率を取得し、リストに追加\n",
    "    y_scores_LR = model_LR.predict_proba(X_test_standard_LR)[:, 1]\n",
    "    predicted_probabilities_LR.append(y_scores_LR)\n",
    "    \n",
    "    # モデルの精度を計算 このモデルの適応度みたいなもの\n",
    "    accuracy_LR = accuracy_score(y_test_LR, y_pred_LR)\n",
    "    accuracies_LR.append(accuracy_LR)\n",
    "\n",
    "# 平均精度を計算\n",
    "average_accuracy_LR = np.mean(accuracies_LR)\n",
    "print(f'Average Accuracy over 10 runs: {average_accuracy_LR:.2f}')\n",
    "\n",
    "from sklearn.model_selection import cross_val_score   #一般化敵には、max_iterは10000の方がよい。収束している\n",
    "accuracies_LR = cross_val_score(model_LR, X_train_standard_LR, y_train_LR, cv=10)\n",
    "print(f'Average Accuracy over 10 folds: {np.mean(accuracies_LR):.2f}')\n",
    "\n",
    "# 特徴量の重要度の平均を計算\n",
    "mean_importances_LR = np.mean(importances_list_LR, axis=0)\n",
    "# 特徴量の名前と重要度の平均をデータフレームにまとめる\n",
    "feature_importance_df_LR = pd.DataFrame({\n",
    "    'Feature': X_train_LR.columns,\n",
    "    'Importance': mean_importances_LR\n",
    "})\n",
    "\n",
    "# 重要度でソート\n",
    "feature_importance_df_LR = feature_importance_df_LR.sort_values(by='Importance', ascending=False)\n",
    "# 上位13項目を抽出\n",
    "top_13_importance_df_LR = feature_importance_df_LR.head(13)\n",
    "\n",
    "# グラフの作成\n",
    "plt.figure(figsize=(20, 16))\n",
    "plt.barh(top_13_importance_df_LR['Feature'], top_13_importance_df_LR['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 13 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "y_scores_LR = model_LR.predict_proba(X_test_standard_LR)[:, 1]  # クラス1の確率を取得\n",
    "#print(y_scores)\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_LR = np.sort(np.unique(y_scores_LR))\n",
    "# 感度（TPR）と特異度を計算\n",
    "tpr_list_LR = []\n",
    "specificity_list_LR = []\n",
    "\n",
    "for threshold in thresholds_LR:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_LR = (y_scores_LR >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_LR, fp_LR, fn_LR, tp_LR = confusion_matrix(y_test_LR, y_pred_LR).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_LR = tp_LR / (tp_LR + fn_LR) if (tp_LR + fn_LR) > 0 else 0  # 感度\n",
    "    specificity_LR = tn_LR / (tn_LR + fp_LR) if (tn_LR + fp_LR) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_LR.append(tpr_LR)\n",
    "    specificity_list_LR.append(specificity_LR)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_LR = auc(np.array(specificity_list_LR), tpr_list_LR)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( np.array(specificity_list_LR), tpr_list_LR, label=f\"ROC Curve (AUC = {roc_auc_LR:.2f})\", color=\"blue\")\n",
    "plt.plot([1, 0], [0, 1], linestyle=\"--\", color=\"red\", label=\"Random Guess\")\n",
    "\n",
    "# グラフの装飾\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に反転\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"Specificity \")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve with TP, FP, TN, FN\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# 混同行列の計算\n",
    "tn_LR, fp_LR, fn_LR, tp_LR = confusion_matrix(y_test_LR, y_pred_LR).ravel()\n",
    "\n",
    "# 感度と特異度の計算\n",
    "sensitivity_LR = tp_LR / (tp_LR + fn_LR)  # 真陽性率\n",
    "specificity_LR = tn_LR / (tn_LR + fp_LR)  # 真陰性率\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity_LR:.4f}\")\n",
    "print(f\"Specificity: {specificity_LR:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6085414-ca63-4961-a712-6ff057ce3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# ファイル名を実際の訓練・テストデータファイル名に置き換えてください\n",
    "train_data_xgb = pd.read_csv('selected_train_set.csv')\n",
    "test_data_xgb = pd.read_csv('selected_test_set.csv')\n",
    "index_col=['id']\n",
    "# 特徴量（X）と目標変数（y）の設定\n",
    "X_train_xgb = train_data_xgb.drop(columns=['PassengerId','Survived'])  # 'target_column'を目標変数の列名に置き換えてください\n",
    "y_train_xgb = train_data_xgb['Survived']\n",
    "X_test_xgb = test_data_xgb.drop(columns=['PassengerId','Survived'])\n",
    "y_test_xgb = test_data_xgb['Survived']\n",
    "\n",
    "# クロスバリデーションで得た最適なブーストラウンド数\n",
    "optimal_boost_rounds = 28  # ここで得た値\n",
    "\n",
    "# クラスの不均衡を考慮した重みの計算\n",
    "ratio = y_train_xgb.value_counts()[0] / y_train_xgb.value_counts()[1]  # 負例/正例の比率\n",
    "\n",
    "# 10回ループの準備\n",
    "accuracies_xgb = []  # 精度を格納するリスト\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    # 訓練-検証の分割\n",
    "    X_train_split_xgb, X_val_split_xgb, y_train_split_xgb, y_val_split_xgb = train_test_split(X_train_xgb, y_train_xgb, test_size=0.2, random_state=42)\n",
    "\n",
    "#以下に新しく提案されたコードを各\n",
    "    # DMatrixに変換\n",
    "    dtrain_xgb = xgb.DMatrix(X_train_split_xgb, label=y_train_split_xgb)\n",
    "    dval_xgb = xgb.DMatrix(X_val_split_xgb, label=y_val_split_xgb)\n",
    "    dtest_xgb = xgb.DMatrix(X_test_xgb)\n",
    "\n",
    "    params = {\n",
    "        'objective':'binary:logistic',\n",
    "        'eval_metric':'logloss',\n",
    "        'random_state':42,\n",
    "        'learning_rate':0.1,\n",
    "        'max_depth':5,\n",
    "        'subsample':1.0,\n",
    "        'colsample_bytree':1.0,\n",
    "        'gamma':0.1\n",
    "}\n",
    "# アーリーストッピングでモデルを訓練\n",
    "    evals_xgb = [(dtrain_xgb, 'train'), (dval_xgb, 'eval')]\n",
    "    model_xgb = xgb.train(params, dtrain_xgb, num_boost_round=optimal_boost_rounds, early_stopping_rounds=10, evals=evals_xgb, verbose_eval=False)\n",
    "    #num_boost_roundを100(0.51)から200に、learn_rateを0.1から0.05に（過学習抑制）、max_depthを3からまずは4に変更\n",
    "    # 予測\n",
    "    y_test_pred_xgb = model_xgb.predict(dtest_xgb)\n",
    "    y_test_pred_xgb = [1 if pred > 0.5 else 0 for pred in y_test_pred_xgb]  # 二値分類として閾値0.5で予測値を変換\n",
    "    accuracy_xgb = accuracy_score(y_test_xgb, y_test_pred_xgb)\n",
    "    accuracies_xgb.append(accuracy_xgb)\n",
    "\n",
    "# 平均精度を計算\n",
    "average_accuracy_xgb = np.mean(accuracies_xgb)\n",
    "print(f'Average Accuracy over 10 runs: {average_accuracy_xgb:.2f}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# 特徴量の重要度を取得 (weightとgain)\n",
    "importance_weight_xgb = model_xgb.get_score(importance_type='weight')\n",
    "\n",
    "# 全ての特徴量を取得してデータフレームにまとめる\n",
    "all_features_xgb = X_train_split_xgb.columns\n",
    "importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': all_features_xgb,\n",
    "    'Weight': [importance_weight_xgb.get(feature, 0) for feature in all_features_xgb],  # Weightがない場合は0\n",
    "})\n",
    "\n",
    "\n",
    "# データフレームにまとめて表示\n",
    "importance_df_xgb = pd.DataFrame({\n",
    "    'Feature': list(importance_weight_xgb.keys()),\n",
    "    'Weight': list(importance_weight_xgb.values()),\n",
    "   \n",
    "})\n",
    "\n",
    "# Weightで上位13項目をソート\n",
    "top_13_weight_df_xgb = importance_df_xgb.sort_values(by='Weight', ascending=False).head(13)\n",
    "\n",
    "\n",
    "# 結果の表示\n",
    "print(\"Top 13 Feature Importances by Weight:\\n\", top_13_weight_df_xgb)\n",
    "\n",
    "\n",
    "# グラフの作成\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Weightのプロット\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(top_13_weight_df_xgb['Feature'], top_13_weight_df_xgb['Weight'], color='lightgreen')\n",
    "plt.xlabel('Weight')\n",
    "plt.title('Top 13 Feature Importances by Weight')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix\n",
    "\n",
    "# テストデータをDMatrix形式に変換\n",
    "dtest = xgb.DMatrix(X_test_xgb)\n",
    "# テストデータでの予測確率を取得\n",
    "y_scores_xgb = model_xgb.predict(dtest_xgb)\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_xgb = np.sort(np.unique(y_scores_xgb))\n",
    "\n",
    "# 感度（TPR）と特異度を計算\n",
    "tpr_list_xgb = []\n",
    "specificity_list_xgb = []\n",
    "\n",
    "for threshold in thresholds_xgb:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_xgb = (y_scores_xgb >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_xgb, fp_xgb, fn_xgb, tp_xgb = confusion_matrix(y_test_xgb, y_pred_xgb).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_xgb = tp_xgb / (tp_xgb + fn_xgb) if (tp_xgb + fn_xgb) > 0 else 0  # 感度\n",
    "    specificity_xgb = tn_xgb / (tn_xgb + fp_xgb) if (tn_xgb + fp_xgb) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_xgb.append(tpr_xgb)\n",
    "    specificity_list_xgb.append(specificity_xgb)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_xgb = auc(1 - np.array(specificity_list_xgb), tpr_list_xgb)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.array(specificity_list_xgb), tpr_list_xgb, label=f\"ROC Curve (AUC = {roc_auc_xgb:.2f})\", color=\"blue\")\n",
    "plt.plot([1, 0], [0, 1], color=\"gray\", linestyle=\"--\")  # 45度線\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に設定\n",
    "plt.ylim([0.0, 1.0])  # Y軸を0から1に設定\n",
    "plt.xlabel(\"Specificity\")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve (Specificity vs Sensitivity)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19424565-d7cd-4558-b18d-eba426bbc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# ファイル名を実際の訓練・テストデータファイル名に置き換えてください\n",
    "train_data_rf = pd.read_csv('selected_train_set.csv')\n",
    "test_data_rf = pd.read_csv('selected_test_set.csv')\n",
    "index_col=['id']\n",
    "# 特徴量（X）と目標変数（y）の設定\n",
    "X_train_rf = train_data_rf.drop(columns=['PassengerId','Survived'])  # 'target_column'を目標変数の列名に置き換えてください\n",
    "y_train_rf = train_data_rf['Survived']\n",
    "X_test_rf = test_data_rf.drop(columns=['PassengerId','Survived'])\n",
    "y_test_rf = test_data_rf['Survived']\n",
    "\n",
    "# スケーリングとモデルのパイプライン設定\n",
    "pipeline_rf = Pipeline([\n",
    "    #('scaler', StandardScaler()),RFのみの時は、下のコードのみでよい\n",
    "    ('model', RandomForestClassifier(n_estimators=200, random_state=42, max_depth=5, min_samples_split=5))\n",
    "])\n",
    "\n",
    "\n",
    "# 10回ループするためのリストを初期化\n",
    "accuracies_rf = []  # 精度を格納するリスト\n",
    "importances_list_rf = []  # 特徴量の重要度を格納するリスト\n",
    "# 10回ループでモデルを作成\n",
    "for i in range(10):\n",
    "    # ランダムフォレストモデルの作成\n",
    "    model_rf = RandomForestClassifier(n_estimators=200, random_state=i,max_depth=5, min_samples_split=5)\n",
    "    \n",
    "    # モデルの訓練\n",
    "    model_rf.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "    # テストデータでの予測\n",
    "    y_pred_rf = model_rf.predict(X_test_rf)\n",
    "    \n",
    "    # モデルの精度を計算\n",
    "    accuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\n",
    "    accuracies_rf.append(accuracy_rf)\n",
    "    \n",
    "    # 特徴量の重要度を取得してリストに追加\n",
    "    importances_rf = model_rf.feature_importances_\n",
    "    importances_list_rf.append(importances_rf)\n",
    "\n",
    "# 平均精度を計算\n",
    "average_accuracy_rf = np.mean(accuracies_rf)\n",
    "print(f'Average Accuracy over 10 runs: {average_accuracy_rf:.2f}')\n",
    "\n",
    "# 特徴量の重要度の平均を計算\n",
    "mean_importances_rf = np.mean(importances_list_rf, axis=0)\n",
    "# 特徴量の名前と重要度の平均をデータフレームにまとめる\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': X_train_rf.columns,\n",
    "    'Importance': mean_importances_rf\n",
    "})\n",
    "\n",
    "# 重要度でソートし、上位13項目を抽出\n",
    "top_13_features_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False).head(13)\n",
    "\n",
    "# プロット\n",
    "plt.barh(top_13_features_rf['Feature'], top_13_features_rf['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Average Feature Importance over 10 runs (Random Forest)')\n",
    "plt.gca().invert_yaxis()  # 特徴量を重要度が高い順に表示\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix\n",
    "# テストデータで予測確率を取得\n",
    "y_scores_rf = model_rf.predict_proba(X_test_rf)[:, 1]\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_rf = np.sort(np.unique(y_scores_rf))\n",
    "\n",
    "# 感度（TPR）と特異度を計算\n",
    "tpr_list_rf = []\n",
    "specificity_list_rf = []\n",
    "\n",
    "for threshold in thresholds_rf:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_rf = (y_scores_rf >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(y_test_rf, y_pred_rf).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_rf = tp_rf / (tp_rf+ fn_rf) if (tp_rf + fn_rf) > 0 else 0  # 感度\n",
    "    specificity_rf = tn_rf / (tn_rf + fp_rf) if (tn_rf + fp_rf) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_rf.append(tpr_rf)\n",
    "    specificity_list_rf.append(specificity_rf)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_rf = auc(np.array(specificity_list_rf), tpr_list_rf)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( np.array(specificity_list_rf), tpr_list_rf, label=f\"ROC Curve (AUC = {roc_auc_rf:.2f})\", color=\"blue\")\n",
    "plt.plot([1, 0], [0, 1], color=\"red\", linestyle=\"--\")  # 45度線\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に設定\n",
    "plt.xlabel(\"Specificity \")\n",
    "plt.ylabel(\"Sensitivity \")\n",
    "plt.title(\"ROC Curve (Specificity vs Sensitivity)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4392597-cffb-4d20-a5fc-68ec81cb2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの読み込み\n",
    "train_data_nn = pd.read_csv('selected_train_set.csv')\n",
    "test_data_nn = pd.read_csv('selected_test_set.csv')\n",
    "\n",
    "# 特徴量と目的変数の分離\n",
    "X_train_nn = train_data_nn.drop(columns=['PassengerId','Survived'])\n",
    "y_train_nn = train_data_nn['Survived']\n",
    "X_test_nn = test_data_nn.drop(columns=['PassengerId','Survived'])\n",
    "y_test_nn = test_data_nn['Survived']\n",
    "\n",
    "# データの標準化\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_scaled_nn = scaler_nn.fit_transform(X_train_nn)\n",
    "X_test_scaled_nn = scaler_nn.transform(X_test_nn)\n",
    "\n",
    "# 標準化されたデータをデータフレームに戻し、元のカラム名を保持\n",
    "X_train_scaled_df_nn = pd.DataFrame(X_train_scaled_nn, columns=X_train_nn.columns, index=X_train_nn.index)\n",
    "X_test_scaled_df_nn = pd.DataFrame(X_test_scaled_nn, columns=X_test_nn.columns, index=X_test_nn.index)\n",
    "# ニューラルネットワークのパラメータグリッドの設定\n",
    "param_grid_nn = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # 隠れ層の構成\n",
    "    'activation': ['relu', 'tanh'],                   # 活性化関数\n",
    "    'learning_rate_init': [0.001, 0.01],              # 学習率\n",
    "    'max_iter': [200, 500]                            # 最大エポック数\n",
    "}\n",
    "\n",
    "# 重要度を格納するリスト\n",
    "importances_list_nn = []\n",
    "# 予測確率を格納するリスト\n",
    "predicted_probabilities_nn = []\n",
    "# 精度を格納するリスト\n",
    "accuracy_scores_nn = []\n",
    "\n",
    "# モデルのトレーニングと評価を10回繰り返す\n",
    "for i in range(10):\n",
    "    # ニューラルネットワークモデルの初期化\n",
    "    model_nn = MLPClassifier(random_state=42)\n",
    "\n",
    "\n",
    "    # GridSearchCVの設定\n",
    "    grid_search_nn = GridSearchCV(estimator=model_nn, param_grid=param_grid_nn, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# 訓練データでモデルをトレーニングし、最適なハイパーパラメータを取得\n",
    "    grid_search_nn.fit(X_train_scaled_df_nn, y_train_nn)\n",
    "    best_model_nn = grid_search_nn.best_estimator_\n",
    "\n",
    "    \n",
    "    # 訓練データでモデルをトレーニング\n",
    "    #model_nn.fit(X_train_scaled_df_nn, y_train_nn)\n",
    "\n",
    "    # 特徴量の重要度を取得し、リストに追加\n",
    "    #importances_list_nn.append(best_model_nn.feature_importances_)\n",
    "\n",
    "    # テストデータで予測確率を取得し、リストに追加\n",
    "    y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "    predicted_probabilities_nn.append(y_scores_nn)\n",
    "\n",
    "    # テストデータで予測を行う\n",
    "    y_pred_nn = best_model_nn.predict(X_test_scaled_df_nn)\n",
    "# Permutation Importanceの計算\n",
    "    result_nn = permutation_importance(best_model_nn, X_test_scaled_df_nn, y_test_nn, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "    importances_list_nn.append(result_nn.importances_mean)\n",
    "\n",
    "    # テストデータで予測確率を取得し、リストに追加\n",
    "    y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "    predicted_probabilities_nn.append(y_scores_nn)\n",
    "\n",
    "    # テストデータで予測を行う\n",
    "    y_pred_nn = best_model_nn.predict(X_test_scaled_df_nn)\n",
    "    \n",
    "# 精度を計算してリストに追加\n",
    "    accuracy_nn = accuracy_score(y_test_nn, y_pred_nn)\n",
    "    accuracy_scores_nn.append(accuracy_nn)\n",
    "\n",
    "# 10回の平均精度を計算\n",
    "average_accuracy_nn = np.mean(accuracy_scores_nn)\n",
    "\n",
    "# 平均精度を出力\n",
    "print(f\"\\n平均精度 (10回): {average_accuracy_nn:.4f}\")\n",
    "\n",
    "\n",
    "# 特徴量の重要度の平均を計算\n",
    "mean_importances_nn = np.mean(importances_list_nn, axis=0)\n",
    "\n",
    "# 特徴量の名前と重要度の平均をデータフレームにまとめる\n",
    "feature_importance_df_nn = pd.DataFrame({\n",
    "    'Feature': X_train_nn.columns,\n",
    "    'Importance': mean_importances_nn\n",
    "})\n",
    "\n",
    "# 重要度でソート\n",
    "feature_importance_df_nn = feature_importance_df_nn.sort_values(by='Importance', ascending=False)\n",
    "# 上位13項目を抽出\n",
    "top_13_importance_df_nn = feature_importance_df_nn.head(13)\n",
    "\n",
    "# グラフの作成\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_13_importance_df_nn['Feature'], top_13_importance_df_nn['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 13 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# 平均予測確率を計算\n",
    "mean_predicted_probabilities_nn = np.mean(predicted_probabilities_nn, axis=0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "\n",
    "# テストデータで予測確率を取得\n",
    "y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_nn = np.sort(np.unique(y_scores_nn))\n",
    "\n",
    "# 感度（TPR）と特異度を計算\n",
    "tpr_list_nn = []\n",
    "specificity_list_nn = []\n",
    "\n",
    "for threshold in thresholds_nn:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_nn = (y_scores_nn >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_nn, fp_nn, fn_nn, tp_nn = confusion_matrix(y_test_nn, y_pred_nn).ravel()\n",
    "\n",
    "    # 感度と特異度を計算\n",
    "    tpr_nn = tp_nn / (tp_nn+ fn_nn) if (tp_nn + fn_nn) > 0 else 0  # 感度\n",
    "    specificity_nn = tn_nn / (tn_nn + fp_nn) if (tn_nn + fp_nn) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_nn.append(tpr_nn)\n",
    "    specificity_list_nn.append(specificity_nn)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_nn = auc(np.array(specificity_list_nn), tpr_list_nn)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( np.array(specificity_list_nn), tpr_list_nn, label=f\"ROC Curve (AUC = {roc_auc_nn:.2f})\", color=\"blue\")\n",
    "\n",
    "plt.plot([1, 0], [0, 1], linestyle=\"--\", color=\"red\", label=\"Random Guess\")\n",
    "\n",
    "# グラフの装飾\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に反転\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel(\"Specificity \")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve with TP, FP, TN, FN\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# 混同行列の計算\n",
    "tn_nn, fp_nn, fn_nn, tp_nn = confusion_matrix(y_test_nn, y_pred_nn).ravel()\n",
    "\n",
    "# 感度と特異度の計算\n",
    "sensitivity_nn = tp_nn / (tp_nn + fn_nn)  # 真陽性率\n",
    "specificity_nn = tn_nn / (tn_nn + fp_nn)  # 真陰性率\n",
    "\n",
    "print(f\"Sensitivity: {sensitivity_nn:.4f}\")\n",
    "print(f\"Specificity: {specificity_nn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76451d88-cc7b-43e9-b5fb-f49d29c620c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# データの読み込み\n",
    "train_data_senkei = pd.read_csv('selected_train_set.csv')\n",
    "test_data_senkei = pd.read_csv('selected_test_set.csv')\n",
    "\n",
    "# 特徴量（X）と目標変数（y）の設定\n",
    "X_train_senkei = train_data_senkei.drop(columns=['PassengerId','Survived'])\n",
    "y_train_senkei = train_data_senkei['Survived']\n",
    "X_test_senkei = test_data_senkei.drop(columns=['PassengerId','Survived'])\n",
    "y_test_senkei = test_data_senkei['Survived']\n",
    "\n",
    "# データの標準化\n",
    "scaler_senkei = StandardScaler()\n",
    "X_train_senkei = scaler_senkei.fit_transform(X_train_senkei)\n",
    "X_test_senkei = scaler_senkei.transform(X_test_senkei)\n",
    "\n",
    "param_grid_linear_senkei = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search_linear_senkei = GridSearchCV(SVC(kernel='linear', probability=True, random_state=42), param_grid_linear_senkei, cv=10, scoring='accuracy')\n",
    "grid_search_linear_senkei.fit(X_train_senkei, y_train_senkei)\n",
    "best_model_senkei = grid_search_linear_senkei.best_estimator_\n",
    "\n",
    "# 線形カーネルのSVMモデルを10回評価 class_weight='balanced'不均衡データに対応するコード0.5→あまり変わらず\n",
    "linear_accuracies_senkei = []\n",
    "\n",
    "for i in range(10):\n",
    "    svm_linear_senkei = SVC(kernel='linear', probability=True, random_state=42,class_weight='balanced')\n",
    "    svm_linear_senkei.fit(X_train_senkei, y_train_senkei)\n",
    "    y_pred_senkei = best_model_senkei.predict(X_test_senkei)\n",
    "    #y_pred_linear_senkei = svm_linear_senkei.predict(X_test_svm_senkei)\n",
    "    accuracy_senkei = accuracy_score(y_test_senkei, y_pred_senkei)\n",
    "    linear_accuracies_senkei.append(accuracy_senkei)\n",
    "\n",
    "# 線形カーネルSVMの平均精度の表示\n",
    "print(\"線形カーネルSVMの10回の平均精度:\", np.mean(linear_accuracies_senkei))\n",
    "print(\"線形カーネルSVMの詳細な評価結果:\\n\", classification_report(y_test_senkei, y_pred_senkei))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# 特徴量の重要度の計算\n",
    "importances_senkei = best_model_senkei.coef_[0]  # 線形SVMの係数が重要度として使える\n",
    "feature_importance_df_senkei = pd.DataFrame({\n",
    "    'Feature': train_data_senkei.drop(columns=['PassengerId','Survived']).columns,\n",
    "    'Importance': np.abs(importances_senkei)  # 絶対値を取って重要度を見やすくする\n",
    "})\n",
    "\n",
    "# 重要度でソートして上位13項目を抽出\n",
    "feature_importance_df_senkei = feature_importance_df_senkei.sort_values(by='Importance', ascending=False)\n",
    "top_13_importance_df_senkei = feature_importance_df_senkei.head(13)\n",
    "\n",
    "# グラフの作成\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_13_importance_df_senkei['Feature'], top_13_importance_df_senkei['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 13 Feature Importances (SVM Linear Kernel)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# モデルの精度評価\n",
    "y_pred_linear_senkei = best_model_senkei.predict(X_test_senkei)\n",
    "accuracy_linear_senkei = accuracy_score(y_test_senkei, y_pred_senkei)\n",
    "print(\"線形カーネルSVMのテスト精度:\", accuracy_senkei)\n",
    "print(\"線形カーネルSVMの詳細な評価結果:\\n\", classification_report(y_test_senkei, y_pred_senkei))\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix\n",
    "\n",
    "# テストデータで予測確率を取得\n",
    "y_scores_senkei = best_model_senkei.predict_proba(X_test_senkei)[:, 1]\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_senkei = np.sort(np.unique(y_scores_senkei))\n",
    "\n",
    "# 感度（TPR）と特異度を計算\n",
    "tpr_list_senkei = []\n",
    "specificity_list_senkei = []\n",
    "\n",
    "for threshold in thresholds_senkei:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_senkei = (y_scores_senkei >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_senkei, fp_senkei, fn_senkei, tp_senkei = confusion_matrix(y_test_senkei, y_pred_senkei).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_senkei = tp_senkei / (tp_senkei+ fn_senkei) if (tp_senkei + fn_senkei) > 0 else 0  # 感度\n",
    "    specificity_senkei = tn_senkei / (tn_senkei + fp_senkei) if (tn_senkei + fp_senkei) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_senkei.append(tpr_senkei)\n",
    "    specificity_list_senkei.append(specificity_senkei)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_senkei = auc(np.array(specificity_list_senkei), tpr_list_senkei)\n",
    "\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( np.array(specificity_list_senkei), tpr_list_senkei, label=f\"ROC Curve (AUC = {roc_auc_senkei:.2f})\", color=\"blue\")\n",
    "\n",
    "plt.plot([1, 0], [0, 1], color=\"red\", linestyle=\"--\")  # 45度線\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に設定\n",
    "plt.xlabel(\"Specificity\")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve (Specificity vs Sensitivity)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a2cbc-c3b3-428e-a41b-e726c2e4355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc,confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データの読み込み\n",
    "train_data_sig = pd.read_csv('selected_train_set.csv')\n",
    "test_data_sig = pd.read_csv('selected_test_set.csv')\n",
    "\n",
    "# 特徴量（X）と目標変数（y）の設定\n",
    "X_train_sig = train_data_sig.drop(columns=['PassengerId','Survived'])\n",
    "y_train_sig = train_data_sig['Survived']\n",
    "X_test_sig = test_data_sig.drop(columns=['PassengerId','Survived'])\n",
    "y_test_sig = test_data_sig['Survived']\n",
    "\n",
    "# データの標準化\n",
    "scaler_sig = StandardScaler()\n",
    "X_train_sig_scaler = scaler_sig.fit_transform(X_train_sig)\n",
    "X_test_sig_scaler = scaler_sig.transform(X_test_sig)\n",
    "\n",
    "\n",
    "#C:0.001,0.01,0.1,1,10,100,gamma:0.0001,0.001,0.01,0.1,1で0.5\n",
    "param_grid_sig = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10,100],\n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.1,1]\n",
    "}\n",
    "# RBF→sigカーネルのSVMでハイパーパラメータチューニングclass_weight='balanced'を加えて、不均衡データに対応→0.5から0.56になった！！！！！\n",
    "svm_sig = SVC(kernel='sigmoid', probability=True, random_state=42,class_weight='balanced')#これが一番！！！\n",
    "#SVC(kernel='poly', probability=True, random_state=42,class_weight='balanced')0.43エラーも出た\n",
    "#SVC(kernel='rbf', probability=True, random_state=42,class_weight='balanced')0.56 (最初に試した)\n",
    "grid_search_sig = GridSearchCV(svm_sig, param_grid_sig, cv=10, scoring='accuracy')\n",
    "grid_search_sig.fit(X_train_sig_scaler, y_train_sig)\n",
    "# 最適なハイパーパラメータでモデルを再訓練\n",
    "best_sig_model = grid_search_sig.best_estimator_\n",
    "best_sig_model.fit(X_train_sig_scaler, y_train_sig)\n",
    "\n",
    "# RBFカーネルモデルの評価\n",
    "y_pred_sig = best_sig_model.predict(X_test_sig_scaler)\n",
    "accuracy_sig = accuracy_score(y_test_sig, y_pred_sig)\n",
    "\n",
    "# テストデータでの予測確率を取得\n",
    "# `predict_proba`を使用してクラス1（positiveクラス）の確率を取得\n",
    "y_scores_sig = best_sig_model.predict_proba(X_test_sig_scaler)[:, 1]\n",
    "\n",
    "# ROC計算用のしきい値を決定（スコアの一意な値を使用）\n",
    "thresholds_sig= np.sort(np.unique(y_scores_sig))\n",
    "\n",
    "# 感度（TPR）と特異度を計算\n",
    "tpr_list_sig = []\n",
    "specificity_list_sig = []\n",
    "\n",
    "for threshold in thresholds_sig:\n",
    "    # スコアをしきい値で2値化\n",
    "    y_pred_sig = (y_scores_sig >= threshold).astype(int)\n",
    "    \n",
    "    # 混同行列を取得\n",
    "    tn_sig, fp_sig, fn_sig, tp_sig = confusion_matrix(y_test_sig, y_pred_sig).ravel()\n",
    "    \n",
    "    # 感度と特異度を計算\n",
    "    tpr_sig = tp_sig / (tp_sig+ fn_sig) if (tp_sig + fn_sig) > 0 else 0  # 感度\n",
    "    specificity_sig = tn_sig / (tn_sig + fp_sig) if (tn_sig + fp_sig) > 0 else 0  # 特異度\n",
    "    \n",
    "    # リストに保存\n",
    "    tpr_list_sig.append(tpr_sig)\n",
    "    specificity_list_sig.append(specificity_sig)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_sig = auc(np.array(specificity_list_sig), tpr_list_sig)\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( np.array(specificity_list_sig), tpr_list_sig, label=f\"ROC Curve (AUC = {roc_auc_sig:.2f})\", color=\"blue\")\n",
    "\n",
    "plt.plot([1, 0], [0, 1], color=\"gray\", linestyle=\"--\")  # 45度線\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に設定\n",
    "plt.xlabel(\"Specificity \")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.title(\"ROC Curve (Specificity vs Sensitivity)\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# RBFカーネルSVMの結果表示\n",
    "print(\"最適化されたsigmoidカーネルSVMの精度:\", accuracy_sig)\n",
    "print(\"sigmoidカーネルSVM最適パラメータ:\", grid_search_sig.best_params_)\n",
    "print(\"sigmoidカーネルSVMの詳細な評価結果:\\n\", classification_report(y_test_sig, y_pred_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e4ce4-39e1-4e48-b7a0-0844a4b50220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# 各モデルから得られた予測スコア（仮定）\n",
    "y_scores_LR = model_LR.predict_proba(X_test_standard_LR)[:, 1]\n",
    "y_scores_rf = model_rf.predict_proba(X_test_rf)[:, 1]\n",
    "y_scores_xgb = model_xgb.predict(dtest_xgb)\n",
    "y_scores_nn = best_model_nn.predict_proba(X_test_scaled_df_nn)[:, 1]\n",
    "y_scores_senkei = best_model_senkei.predict_proba(X_test_senkei)[:, 1]\n",
    "y_scores_sig = best_sig_model.predict_proba(X_test_sig_scaler)[:, 1]\n",
    "\n",
    "# しきい値を決定（予測スコアの一意な値）\n",
    "thresholds_LR = np.sort(np.unique(y_scores_LR))\n",
    "thresholds_rf = np.sort(np.unique(y_scores_rf))\n",
    "thresholds_xgb = np.sort(np.unique(y_scores_xgb))\n",
    "thresholds_nn = np.sort(np.unique(y_scores_nn))\n",
    "thresholds_senkei = np.sort(np.unique(y_scores_senkei))\n",
    "thresholds_sig = np.sort(np.unique(y_scores_sig))\n",
    "\n",
    "# 感度と特異度のリストを格納\n",
    "tpr_list_LR, specificity_list_LR = [], []\n",
    "tpr_list_rf, specificity_list_rf = [], []\n",
    "tpr_list_xgb, specificity_list_xgb = [], []\n",
    "tpr_list_nn, specificity_list_nn = [], []\n",
    "tpr_list_senkei, specificity_list_senkei = [], []\n",
    "tpr_list_sig, specificity_list_sig = [], []\n",
    "\n",
    "# 各モデルにおけるしきい値に対する感度と特異度を計算\n",
    "for threshold in thresholds_LR:\n",
    "    y_pred_LR = (y_scores_LR >= threshold).astype(int)\n",
    "    tn_LR, fp_LR, fn_LR, tp_LR = confusion_matrix(y_test_LR, y_pred_LR).ravel()\n",
    "    tpr_LR = tp_LR / (tp_LR + fn_LR) if (tp_LR + fn_LR) > 0 else 0  # 感度\n",
    "    specificity_LR = tn_LR / (tn_LR + fp_LR) if (tn_LR + fp_LR) > 0 else 0  # 特異度\n",
    "    tpr_list_LR.append(tpr_LR)\n",
    "    specificity_list_LR.append(specificity_LR)\n",
    "\n",
    "for threshold in thresholds_rf:\n",
    "    y_pred_rf = (y_scores_rf >= threshold).astype(int)\n",
    "    tn_rf, fp_rf, fn_rf, tp_rf = confusion_matrix(y_test_rf, y_pred_rf).ravel()\n",
    "    tpr_rf = tp_rf / (tp_rf + fn_rf) if (tp_rf + fn_rf) > 0 else 0  # 感度\n",
    "    specificity_rf = tn_rf / (tn_rf + fp_rf) if (tn_rf + fp_rf) > 0 else 0  # 特異度\n",
    "    tpr_list_rf.append(tpr_rf)\n",
    "    specificity_list_rf.append(specificity_rf)\n",
    "\n",
    "for threshold in thresholds_xgb:\n",
    "    y_pred_xgb = (y_scores_xgb >= threshold).astype(int)\n",
    "    tn_xgb, fp_xgb, fn_xgb, tp_xgb = confusion_matrix(y_test_xgb, y_pred_xgb).ravel()\n",
    "    tpr_xgb = tp_xgb / (tp_xgb + fn_xgb) if (tp_xgb + fn_xgb) > 0 else 0  # 感度\n",
    "    specificity_xgb = tn_xgb / (tn_xgb + fp_xgb) if (tn_xgb + fp_xgb) > 0 else 0  # 特異度\n",
    "    tpr_list_xgb.append(tpr_xgb)\n",
    "    specificity_list_xgb.append(specificity_xgb)\n",
    "\n",
    "for threshold in thresholds_nn:\n",
    "    y_pred_nn = (y_scores_nn >= threshold).astype(int)\n",
    "    tn_nn, fp_nn, fn_nn, tp_nn = confusion_matrix(y_test_nn, y_pred_nn).ravel()\n",
    "    tpr_nn = tp_nn / (tp_nn + fn_nn) if (tp_nn + fn_nn) > 0 else 0  # 感度\n",
    "    specificity_nn = tn_nn / (tn_nn + fp_nn) if (tn_nn + fp_nn) > 0 else 0  # 特異度\n",
    "    tpr_list_nn.append(tpr_nn)\n",
    "    specificity_list_nn.append(specificity_nn)\n",
    "\n",
    "for threshold in thresholds_senkei:\n",
    "    y_pred_senkei = (y_scores_senkei >= threshold).astype(int)\n",
    "    tn_senkei, fp_senkei, fn_senkei, tp_senkei = confusion_matrix(y_test_senkei, y_pred_senkei).ravel()\n",
    "    tpr_senkei = tp_senkei / (tp_senkei + fn_senkei) if (tp_senkei + fn_senkei) > 0 else 0  # 感度\n",
    "    specificity_senkei = tn_senkei / (tn_senkei + fp_senkei) if (tn_senkei + fp_senkei) > 0 else 0  # 特異度\n",
    "    tpr_list_senkei.append(tpr_senkei)\n",
    "    specificity_list_senkei.append(specificity_senkei)\n",
    "\n",
    "for threshold in thresholds_sig:\n",
    "    y_pred_sig = (y_scores_sig >= threshold).astype(int)\n",
    "    tn_sig, fp_sig, fn_sig, tp_sig = confusion_matrix(y_test_sig, y_pred_sig).ravel()\n",
    "    tpr_sig = tp_sig / (tp_sig + fn_sig) if (tp_sig + fn_sig) > 0 else 0  # 感度\n",
    "    specificity_sig = tn_sig / (tn_sig + fp_sig) if (tn_sig + fp_sig) > 0 else 0  # 特異度\n",
    "    tpr_list_sig.append(tpr_sig)\n",
    "    specificity_list_sig.append(specificity_sig)\n",
    "\n",
    "# AUCの計算\n",
    "roc_auc_LR = auc(np.array(specificity_list_LR), np.array(tpr_list_LR))\n",
    "roc_auc_rf = auc(np.array(specificity_list_rf), np.array(tpr_list_rf))\n",
    "roc_auc_xgb = auc(np.array(specificity_list_xgb), np.array(tpr_list_xgb))\n",
    "roc_auc_nn = auc(np.array(specificity_list_nn), np.array(tpr_list_nn))\n",
    "roc_auc_senkei = auc(np.array(specificity_list_senkei), np.array(tpr_list_senkei))\n",
    "roc_auc_sig = auc(np.array(specificity_list_sig), np.array(tpr_list_sig))\n",
    "\n",
    "plt.rcParams['font.family'] = 'MS Gothic'  # 日本語対応フォントを指定\n",
    "# ROC曲線を描画\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 各モデルのROC曲線を描画\n",
    "plt.plot(specificity_list_LR, tpr_list_LR, label=f\"ロジスティック回帰 (AUC = {roc_auc_LR:.2f})\", color=\"blue\",lw=3,linestyle='-')\n",
    "plt.plot(specificity_list_rf, tpr_list_rf, label=f\"ランダムフォレスト (AUC = {roc_auc_rf:.2f})\", color=\"green\",lw=3,linestyle='-')\n",
    "plt.plot(specificity_list_xgb, tpr_list_xgb, label=f\"XGBoost (AUC = {roc_auc_xgb:.2f})\", color=\"red\",lw=3,linestyle='-')\n",
    "plt.plot(specificity_list_nn, tpr_list_nn, label=f\"ニューラルネットワーク (AUC = {roc_auc_nn:.2f})\", color=\"purple\",lw=3,linestyle='-')\n",
    "plt.plot(specificity_list_senkei, tpr_list_senkei, label=f\"線形サポートベクターマシン (AUC = {roc_auc_senkei:.2f})\", color='#3C3C3C',lw=3,linestyle='--')\n",
    "plt.plot(specificity_list_sig, tpr_list_sig, label=f\"非線形サポートベクターマシン (AUC = {roc_auc_sig:.2f})\", color='#7D7D7D',lw=3,linestyle='--')\n",
    "\n",
    "# 45度線を描画\n",
    "plt.plot([1, 0], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "\n",
    "# 軸の設定\n",
    "plt.xlim([1.0, 0.0])  # X軸を1から0に設定\n",
    "plt.xlabel('特異度',fontsize=24)\n",
    "plt.ylabel('感度',fontsize=24)\n",
    "plt.title('ROC曲線（訓練テスト=8:2）',fontsize=26)\n",
    "plt.legend(loc='lower right',title=\"Models\",fontsize=13)\n",
    "\n",
    "# 目盛りのフォントサイズ\n",
    "plt.xticks(fontsize=18)  # X軸の目盛り\n",
    "plt.yticks(fontsize=18)  # Y軸の目盛り\n",
    "# グリッドを表示\n",
    "plt.grid()\n",
    "\n",
    "# グラフを表示\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287196f-bd01-4efe-b05d-7b63194884ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
